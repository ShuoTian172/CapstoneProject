{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Immigration Insights\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This Project creates a Data Lake type of ETL pipeline to process, clean and store data related to US I94 Immigration data using Spark SQL.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *\n",
    "import re\n",
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Scope of the project is to create an ETL pipeline for processing, cleaning and storing data related to US I94 immigration data, and country codes, processed data stored in Star schema model to parquet files. \n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "* **World Temperature Data**: comes from kaggle and includes data on temperature changes in the U.S. since 1850.\n",
    "    * Source: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "\n",
    "* **data/18-83510-I94-Data-2016/**: US I94 immigration data from 2016 (Jan-Dec).\n",
    "    * Source: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "    * Description: I94_SAS_Labels_Descriptions.txt file contains descriptions for the I94 data.\n",
    "        * I94 dataset has SAS7BDAT file per each month of the year (e.g. i94_jan16_sub.sas7bdat).\n",
    "        * Each file contains about 3M rows\n",
    "    * Data is available for Udacity DEND course\n",
    "    \n",
    "* **data/i94_airport_codes.xlsx**: Airport codes and related cities defined in I94 data description file.\n",
    "    * Source: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "    * Description: I94 Airport codes data contains information about different airports around the world.\n",
    "    * NOTE: I94 data uses its own codes for airports instead of using standard codes (like IATA). Therefore, I94 airport codes have been taken from I94 data description file and processed for ETL use.  \n",
    "\n",
    "* **data/i94_country_codes.xlsx**: Country codes defined in US I94 Immigration data description file. \n",
    "    * Source: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "    * Description: I94 Country codes data contains information about countries people come to US from.\n",
    "    * NOTE: I94 data uses its own codes for countries instead of using ISO-3166 standard codes. Therefore, I94 country codes have been taken from I94 data description file and processed for ETL use.\n",
    "  \n",
    "* **data/airport-codes.csv**: Airport codes and related cities.\n",
    "    * Source: https://datahub.io/core/airport-codes#data\n",
    "    * Description: Airpot codes data contains information about different airports around the world.\n",
    "\n",
    "* **data/iso-3166-country-codes.json**: World country codes (ISO-3166)\n",
    "    * Source: https://www.iso.org/iso-3166-country-codes.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "### Explore the Data\n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "### Cleaning Steps\n",
    "\n",
    "- World Temperature Data:\n",
    "    * Filter average temperature data only for year == 2013 and create new fields with year, month, fahrenheit and run abbreviations function and drop duplicates.\n",
    "- I94 data:\n",
    "    * Remove nulls then select important columns from the immigration data and drop duplicates.\n",
    "    \n",
    "- I94 Airport data: \n",
    "    * Remove quote marks and extra white spaces from the data.\n",
    "- I94 Country Code data: \n",
    "    * Remove quote marks and extra white spaces from the data.\n",
    "- ISO Country Code data:\n",
    "    * No action required. Antarctica is handled as a special case to avoid duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build SQL context object\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "airport=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and read I94 Airport code data to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i94_airport_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94port</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'ALC'</th>\n",
       "      <td>'ALCAN, AK             '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'ANC'</th>\n",
       "      <td>'ANCHORAGE, AK         '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'BAR'</th>\n",
       "      <td>'BAKER AAF - BAKER ISLAND, AK'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'DAC'</th>\n",
       "      <td>'DALTONS CACHE, AK     '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'PIZ'</th>\n",
       "      <td>'DEW STATION PT LAY DEW, AK'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        i94_airport_name\n",
       "i94port                                 \n",
       "   'ALC'        'ALCAN, AK             '\n",
       "   'ANC'        'ANCHORAGE, AK         '\n",
       "   'BAR'  'BAKER AAF - BAKER ISLAND, AK'\n",
       "   'DAC'        'DALTONS CACHE, AK     '\n",
       "   'PIZ'    'DEW STATION PT LAY DEW, AK'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read I94 Airport codes data:\n",
    "airport_codes_i94_df = pd.read_excel(\"i94_airport_codes.xlsx\", header=0, index_col=0)\n",
    "airport_codes_i94_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning I94 Airport data first\n",
    "ac = {\"i94port_clean\": [], \"i94_airport_name_clean\": [], \"i94_state_clean\": []}\n",
    "codes = []\n",
    "names = []\n",
    "states = []\n",
    "for index, row in airport_codes_i94_df.iterrows():\n",
    "    y = re.sub(\"'\", \"\", index)\n",
    "    x = re.sub(\"'\", \"\", row[0])\n",
    "    z = re.sub(\"'\", \"\", row[0]).split(\",\")\n",
    "    y = y.strip()\n",
    "    z[0] = z[0].strip()\n",
    "    \n",
    "    if len(z) == 2:\n",
    "        codes.append(y)\n",
    "        names.append(z[0])\n",
    "        z[1] = z[1].strip()\n",
    "        states.append(z[1])\n",
    "    else:\n",
    "        codes.append(y)\n",
    "        names.append(z[0])\n",
    "        states.append(\"NaN\")\n",
    "\n",
    "ac[\"i94port_clean\"] = codes\n",
    "ac[\"i94_airport_name_clean\"] = names\n",
    "ac[\"i94_state_clean\"] = states\n",
    "\n",
    "airport_codes_i94_df_clean = pd.DataFrame.from_dict(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_i94_schema = t.StructType([\n",
    "                            t.StructField(\"i94_port\", t.StringType(), False),\n",
    "                            t.StructField(\"i94_airport_name\", t.StringType(), False),\n",
    "                            t.StructField(\"i94_airport_state\", t.StringType(), False)\n",
    "                        ])\n",
    "new_airport_data = spark.createDataFrame(airport_codes_i94_df_clean, schema=airport_codes_i94_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94_port: string (nullable = false)\n",
      " |-- i94_airport_name: string (nullable = false)\n",
      " |-- i94_airport_state: string (nullable = false)\n",
      "\n",
      "+--------+------------------------+-----------------+\n",
      "|i94_port|i94_airport_name        |i94_airport_state|\n",
      "+--------+------------------------+-----------------+\n",
      "|ALC     |ALCAN                   |AK               |\n",
      "|ANC     |ANCHORAGE               |AK               |\n",
      "|BAR     |BAKER AAF - BAKER ISLAND|AK               |\n",
      "|DAC     |DALTONS CACHE           |AK               |\n",
      "|PIZ     |DEW STATION PT LAY DEW  |AK               |\n",
      "+--------+------------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_airport_data.printSchema()\n",
    "new_airport_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and read ISO Country Code data to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>alpha-2</th>\n",
       "      <th>alpha-3</th>\n",
       "      <th>country-code</th>\n",
       "      <th>iso_3166-2</th>\n",
       "      <th>region</th>\n",
       "      <th>sub-region</th>\n",
       "      <th>intermediate-region</th>\n",
       "      <th>region-code</th>\n",
       "      <th>sub-region-code</th>\n",
       "      <th>intermediate-region-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>4</td>\n",
       "      <td>ISO 3166-2:AF</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Åland Islands</td>\n",
       "      <td>AX</td>\n",
       "      <td>ALA</td>\n",
       "      <td>248</td>\n",
       "      <td>ISO 3166-2:AX</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Northern Europe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>AL</td>\n",
       "      <td>ALB</td>\n",
       "      <td>8</td>\n",
       "      <td>ISO 3166-2:AL</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "      <td>DZA</td>\n",
       "      <td>12</td>\n",
       "      <td>ISO 3166-2:DZ</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Samoa</td>\n",
       "      <td>AS</td>\n",
       "      <td>ASM</td>\n",
       "      <td>16</td>\n",
       "      <td>ISO 3166-2:AS</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>Polynesia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name alpha-2 alpha-3  country-code     iso_3166-2   region  \\\n",
       "0     Afghanistan      AF     AFG             4  ISO 3166-2:AF     Asia   \n",
       "1   Åland Islands      AX     ALA           248  ISO 3166-2:AX   Europe   \n",
       "2         Albania      AL     ALB             8  ISO 3166-2:AL   Europe   \n",
       "3         Algeria      DZ     DZA            12  ISO 3166-2:DZ   Africa   \n",
       "4  American Samoa      AS     ASM            16  ISO 3166-2:AS  Oceania   \n",
       "\n",
       "        sub-region intermediate-region  region-code  sub-region-code  \\\n",
       "0    Southern Asia                 NaN        142.0             34.0   \n",
       "1  Northern Europe                 NaN        150.0            154.0   \n",
       "2  Southern Europe                 NaN        150.0             39.0   \n",
       "3  Northern Africa                 NaN          2.0             15.0   \n",
       "4        Polynesia                 NaN          9.0             61.0   \n",
       "\n",
       "   intermediate-region-code  \n",
       "0                       NaN  \n",
       "1                       NaN  \n",
       "2                       NaN  \n",
       "3                       NaN  \n",
       "4                       NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_codes_df = pd.read_csv('iso-3166-country-codes.csv', header=0)\n",
    "country_codes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code_schema = t.StructType([\n",
    "                            t.StructField(\"name\", t.StringType(), False),\n",
    "                            t.StructField(\"alpha_2\", t.StringType(), False),\n",
    "                            t.StructField(\"alpha_3\", t.StringType(), False),\n",
    "                            t.StructField(\"country_code\", t.StringType(), False),\n",
    "                            t.StructField(\"iso_3166_2\", t.StringType(), False),\n",
    "                            t.StructField(\"region\", t.StringType(), True),\n",
    "                            t.StructField(\"sub_region\", t.StringType(), True),\n",
    "                            t.StructField(\"intermediate_region\", t.StringType(), False),\n",
    "                            t.StructField(\"region_code\", t.StringType(), True),\n",
    "                            t.StructField(\"sub_region_code\", t.StringType(), True),\n",
    "                            t.StructField(\"intermediate_region_code\", t.StringType(), False)\n",
    "                        ])\n",
    "country_codes_iso_df_spark = spark.createDataFrame(country_codes_df, schema=country_code_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- alpha_2: string (nullable = false)\n",
      " |-- alpha_3: string (nullable = false)\n",
      " |-- country_code: string (nullable = false)\n",
      " |-- iso_3166_2: string (nullable = false)\n",
      " |-- region: string (nullable = true)\n",
      " |-- sub_region: string (nullable = true)\n",
      " |-- intermediate_region: string (nullable = false)\n",
      " |-- region_code: string (nullable = true)\n",
      " |-- sub_region_code: string (nullable = true)\n",
      " |-- intermediate_region_code: string (nullable = false)\n",
      "\n",
      "+--------------+-------+-------+------------+-------------+-------+---------------+-------------------+-----------+---------------+------------------------+\n",
      "|name          |alpha_2|alpha_3|country_code|iso_3166_2   |region |sub_region     |intermediate_region|region_code|sub_region_code|intermediate_region_code|\n",
      "+--------------+-------+-------+------------+-------------+-------+---------------+-------------------+-----------+---------------+------------------------+\n",
      "|Afghanistan   |AF     |AFG    |4           |ISO 3166-2:AF|Asia   |Southern Asia  |NaN                |142.0      |34.0           |NaN                     |\n",
      "|Åland Islands |AX     |ALA    |248         |ISO 3166-2:AX|Europe |Northern Europe|NaN                |150.0      |154.0          |NaN                     |\n",
      "|Albania       |AL     |ALB    |8           |ISO 3166-2:AL|Europe |Southern Europe|NaN                |150.0      |39.0           |NaN                     |\n",
      "|Algeria       |DZ     |DZA    |12          |ISO 3166-2:DZ|Africa |Northern Africa|NaN                |2.0        |15.0           |NaN                     |\n",
      "|American Samoa|AS     |ASM    |16          |ISO 3166-2:AS|Oceania|Polynesia      |NaN                |9.0        |61.0           |NaN                     |\n",
      "+--------------+-------+-------+------------+-------------+-------+---------------+-------------------+-----------+---------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_codes_iso_df_spark.printSchema()\n",
    "country_codes_iso_df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and read i94_country_codes to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i94_country_name</th>\n",
       "      <th>iso_country_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94cit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>MEXICO'</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>'AFGHANISTAN'</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>'ALBANIA'</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>'ALGERIA'</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>'ANDORRA'</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       i94_country_name  iso_country_code\n",
       "i94cit                                   \n",
       "582             MEXICO'               484\n",
       "236       'AFGHANISTAN'                 4\n",
       "101           'ALBANIA'                 8\n",
       "316           'ALGERIA'                12\n",
       "102           'ANDORRA'                20"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_codes_i94_df = pd.read_excel('i94_country_codes.xlsx', header=0, index_col=0)\n",
    "country_codes_i94_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning I94 Country Code data first\n",
    "cc = {\"i94cit_clean\": [],\n",
    "      \"i94_country_name_clean\": [],\n",
    "      \"iso_country_code_clean\" : []\n",
    "      }\n",
    "ccodes = []\n",
    "cnames = []\n",
    "ccodes_iso = []\n",
    "\n",
    "for index, row in country_codes_i94_df.iterrows():\n",
    "    cname = re.sub(\"'\", \"\", row[0]).strip()\n",
    "    ccode_iso = row[1]\n",
    "    ccodes.append(index)\n",
    "    cnames.append(cname)\n",
    "    ccodes_iso.append(ccode_iso)\n",
    "\n",
    "cc[\"i94cit_clean\"] = ccodes\n",
    "cc[\"i94_country_name_clean\"] = cnames\n",
    "cc[\"iso_country_code_clean\"] = ccodes_iso\n",
    "country_codes_i94_df_clean = pd.DataFrame.from_dict(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes_i94_schema = t.StructType([\n",
    "                            t.StructField(\"i94_cit\", t.StringType(), False),\n",
    "                            t.StructField(\"i94_country_name\", t.StringType(), False),\n",
    "                            t.StructField(\"iso_country_code\", t.StringType(), False)\n",
    "                        ])\n",
    "country_codes_i94_df_spark = spark.createDataFrame(country_codes_i94_df_clean, schema=country_codes_i94_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94_cit: string (nullable = false)\n",
      " |-- i94_country_name: string (nullable = false)\n",
      " |-- iso_country_code: string (nullable = false)\n",
      "\n",
      "+-------+----------------+----------------+\n",
      "|i94_cit|i94_country_name|iso_country_code|\n",
      "+-------+----------------+----------------+\n",
      "|582    |MEXICO          |484             |\n",
      "|236    |AFGHANISTAN     |4               |\n",
      "|101    |ALBANIA         |8               |\n",
      "|316    |ALGERIA         |12              |\n",
      "|102    |ANDORRA         |20              |\n",
      "+-------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_codes_i94_df_spark.printSchema()\n",
    "country_codes_i94_df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and clean Temperature Data by Country to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatureData=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"GlobalLandTemperaturesByCountry.csv\")\n",
    "\n",
    "#filter the world Temperature Data for year only == 2013 and drop duplicates and convert celcius temp to fahrenheit\n",
    "Temperatures=temperatureData.filter(year(temperatureData[\"dt\"])==2013)\\\n",
    ".withColumn(\"year\",year(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"month\",month(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"avg_temp_fahrenheit\",temperatureData[\"AverageTemperature\"]*9/5+32)\\\n",
    ".withColumn(\"Country\", upper(col(\"Country\")))\n",
    "\n",
    "\n",
    "new_Temperatures=Temperatures.select(\"year\",\"month\",round(col(\"AverageTemperature\"),1).alias(\"avg_temp_celcius\"),\\\n",
    "                                       round(col(\"avg_temp_fahrenheit\"),1).alias(\"avg_temp_fahrenheit\"),\n",
    "                                       \"Country\").dropDuplicates()\n",
    "new_Temperatures_joined = new_Temperatures.join(country_codes_i94_df_spark, \\\n",
    "                                            (country_codes_i94_df_spark.i94_country_name == \\\n",
    "                                                    new_Temperatures.Country))\\\n",
    "                                            .withColumn(\"i94_cit\", col(\"i94_cit\").cast(\"integer\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- avg_temp_celcius: double (nullable = true)\n",
      " |-- avg_temp_fahrenheit: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- i94_cit: integer (nullable = true)\n",
      " |-- i94_country_name: string (nullable = false)\n",
      " |-- iso_country_code: string (nullable = false)\n",
      "\n",
      "+----+-----+----------------+-------------------+-------+-------+----------------+----------------+\n",
      "|year|month|avg_temp_celcius|avg_temp_fahrenheit|Country|i94_cit|i94_country_name|iso_country_code|\n",
      "+----+-----+----------------+-------------------+-------+-------+----------------+----------------+\n",
      "|2013|    7|            21.8|               71.2|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    9|            null|               null|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    8|            21.5|               70.8|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    2|             0.6|               33.1|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    4|            10.5|               50.9|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    5|            15.2|               59.4|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    1|            -2.4|               27.7|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    3|             4.4|               39.9|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    6|            19.1|               66.5|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    4|            25.5|               77.8|BAHAMAS|    512|         BAHAMAS|              44|\n",
      "+----+-----+----------------+-------------------+-------+-------+----------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1710"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Temperatures_joined.printSchema()\n",
    "new_Temperatures_joined.show(10)\n",
    "new_Temperatures_joined.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and read Immigration Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark=spark.read.format('com.github.saurfang.sas.spark').load(\"../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\")\n",
    "\n",
    "i94_data=df_spark.filter(df_spark.i94addr.isNotNull())\\\n",
    ".filter(df_spark.i94res.isNotNull())\\\n",
    ".withColumn(\"i94yr\",col(\"i94yr\").cast(\"integer\"))\\\n",
    ".withColumn(\"i94mon\",col(\"i94mon\").cast(\"integer\"))\\\n",
    ".withColumn(\"i94cit\",col(\"i94cit\").cast(\"integer\"))\\\n",
    "\n",
    "new_I94_Data=i94_data.select(\"cicid\",col(\"i94yr\").alias(\"year\"),col(\"i94mon\").alias(\"month\"),\\\n",
    "                             \"i94port\",col(\"i94addr\").alias(\"state_code\"), col(\"i94port\").alias(\"airport_id\"), col(\"i94cit\").alias(\"country_id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+-------+----------+----------+----------+\n",
      "|cicid|year|month|i94port|state_code|airport_id|country_id|\n",
      "+-----+----+-----+-------+----------+----------+----------+\n",
      "|  7.0|2016|    4|    ATL|        AL|       ATL|       254|\n",
      "| 15.0|2016|    4|    WAS|        MI|       WAS|       101|\n",
      "| 16.0|2016|    4|    NYC|        MA|       NYC|       101|\n",
      "| 17.0|2016|    4|    NYC|        MA|       NYC|       101|\n",
      "| 18.0|2016|    4|    NYC|        MI|       NYC|       101|\n",
      "+-----+----+-----+-------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- country_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_I94_Data.show(5)\n",
    "new_I94_Data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the Data Model\n",
    "## 3.1 Conceptual Data Model\n",
    "### Star Schema\n",
    "\n",
    "### **Data Dictionary Dimension Tables**\n",
    "#### new_airport_data\n",
    " * i94_port: string (nullable = false)\n",
    " * i94_airport_name: string (nullable = false)\n",
    " * i94_airport_state: string (nullable = false)\n",
    "\n",
    " \n",
    "#### country_codes_iso_df_spark\n",
    " * country_code: string (nullable = false)\n",
    " * country_name: string (nullable = false\n",
    " * alpha_2: string (nullable = false)\n",
    " * alpha_3: string (nullable = false)\n",
    " * iso_3166_2: string (nullable = false)\n",
    " * region: string (nullable = true)\n",
    " * sub_region: string (nullable = true)\n",
    " * intermediate_region: string (nullable = false)\n",
    " * region_code: string (nullable = true)\n",
    " * sub_region_code: string (nullable = true)\n",
    " * intermediate_region_code: string (nullable = false)\n",
    " \n",
    "#### new_I94_Data\n",
    " * cicid: double (nullable = true)-ID number of each individual\n",
    " * year: integer (nullable = true)-Year of Immigration\n",
    " * month: integer (nullable = true)-Month of Immigration\n",
    " * i94port: string (nullable = true)-City Port Code where Immigrant entered\n",
    " \n",
    "#### new_Temperatures_joined\n",
    " * year: integer (nullable = true)-Temperature Year\n",
    " * month: integer (nullable = true)-Temerpature Month\n",
    " * avg_temp_celcius: double (nullable = true)-Avg Temperature in Celcius\n",
    " * avg_temp_fahrenheit: double (nullable = true)-Avg Temperatrue in Fahrenheit\n",
    " * Country: string (nullable = true)- Country name\n",
    " * i94_cit: integer (nullable = true)\n",
    " * i94_country_name: string (nullable = false)\n",
    " * iso_country_code: string (nullable = false)\n",
    "\n",
    "\n",
    "### Fact Table (immigrations_table)\n",
    " * cicid: double (nullable = true)-ID number of each individual\n",
    " * avg_temp_fahrenheit: double (nullable = true)-Avg Temperature in Celcius\n",
    " * airport_id: string (nullable = true)\n",
    " * country_id: integer (nullable = true)\n",
    " * country_name: string (nullable = false)\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "1. Dimension tables will be created from cleansed data.\n",
    "2. Fact table is created as a SQL query with joins to dimension tables.\n",
    "3. Fact table is converted back to a spark dataframe.\n",
    "4. Fact table is written as final parquet file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model\n",
    "\n",
    "1. Dimension tables will be created from cleansed data.\n",
    "2. Fact table is created as a SQL query with joins to dimension tables.\n",
    "3. Fact table is converted back to a spark dataframe.\n",
    "4. Fact table is written as final parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create countries table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join tables\n",
    "country_codes_i94_df_spark_joined = country_codes_i94_df_spark\\\n",
    "                                        .join(country_codes_iso_df_spark, \\\n",
    "                                            (country_codes_i94_df_spark.iso_country_code == \\\n",
    "                                                    country_codes_iso_df_spark.country_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_code: string (nullable = false)\n",
      " |-- country_name: string (nullable = false)\n",
      " |-- iso_ccode: string (nullable = false)\n",
      " |-- iso_alpha_2: string (nullable = false)\n",
      " |-- iso_alpha_3: string (nullable = false)\n",
      " |-- iso_3166_2_code: string (nullable = false)\n",
      " |-- iso_country_name: string (nullable = false)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- iso_sub_region: string (nullable = true)\n",
      " |-- iso_region_code: string (nullable = true)\n",
      " |-- iso_sub_region_code: string (nullable = true)\n",
      "\n",
      "+------------+---------------+---------+-----------+-----------+---------------+-------------------+----------+--------------------+---------------+-------------------+\n",
      "|country_code|   country_name|iso_ccode|iso_alpha_2|iso_alpha_3|iso_3166_2_code|   iso_country_name|iso_region|      iso_sub_region|iso_region_code|iso_sub_region_code|\n",
      "+------------+---------------+---------+-----------+-----------+---------------+-------------------+----------+--------------------+---------------+-------------------+\n",
      "|         236|    AFGHANISTAN|        4|         AF|        AFG|  ISO 3166-2:AF|        Afghanistan|      Asia|       Southern Asia|          142.0|               34.0|\n",
      "|         101|        ALBANIA|        8|         AL|        ALB|  ISO 3166-2:AL|            Albania|    Europe|     Southern Europe|          150.0|               39.0|\n",
      "|         316|        ALGERIA|       12|         DZ|        DZA|  ISO 3166-2:DZ|            Algeria|    Africa|     Northern Africa|            2.0|               15.0|\n",
      "|         102|        ANDORRA|       20|         AD|        AND|  ISO 3166-2:AD|            Andorra|    Europe|     Southern Europe|          150.0|               39.0|\n",
      "|         324|         ANGOLA|       24|         AO|        AGO|  ISO 3166-2:AO|             Angola|    Africa|  Sub-Saharan Africa|            2.0|              202.0|\n",
      "|         529|       ANGUILLA|      660|         AI|        AIA|  ISO 3166-2:AI|           Anguilla|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         518|ANTIGUA-BARBUDA|       28|         AG|        ATG|  ISO 3166-2:AG|Antigua and Barbuda|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         687|      ARGENTINA|       32|         AR|        ARG|  ISO 3166-2:AR|          Argentina|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         151|        ARMENIA|       51|         AM|        ARM|  ISO 3166-2:AM|            Armenia|      Asia|        Western Asia|          142.0|              145.0|\n",
      "|         532|          ARUBA|      533|         AW|        ABW|  ISO 3166-2:AW|              Aruba|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         438|      AUSTRALIA|       36|         AU|        AUS|  ISO 3166-2:AU|          Australia|   Oceania|Australia and New...|            9.0|               53.0|\n",
      "|         103|        AUSTRIA|       40|         AT|        AUT|  ISO 3166-2:AT|            Austria|    Europe|      Western Europe|          150.0|              155.0|\n",
      "|         152|     AZERBAIJAN|       31|         AZ|        AZE|  ISO 3166-2:AZ|         Azerbaijan|      Asia|        Western Asia|          142.0|              145.0|\n",
      "|         512|        BAHAMAS|       44|         BS|        BHS|  ISO 3166-2:BS|            Bahamas|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         298|        BAHRAIN|       48|         BH|        BHR|  ISO 3166-2:BH|            Bahrain|      Asia|        Western Asia|          142.0|              145.0|\n",
      "|         274|     BANGLADESH|       50|         BD|        BGD|  ISO 3166-2:BD|         Bangladesh|      Asia|       Southern Asia|          142.0|               34.0|\n",
      "|         513|       BARBADOS|       52|         BB|        BRB|  ISO 3166-2:BB|           Barbados|  Americas|Latin America and...|           19.0|              419.0|\n",
      "|         153|        BELARUS|      112|         BY|        BLR|  ISO 3166-2:BY|            Belarus|    Europe|      Eastern Europe|          150.0|              151.0|\n",
      "|         104|        BELGIUM|       56|         BE|        BEL|  ISO 3166-2:BE|            Belgium|    Europe|      Western Europe|          150.0|              155.0|\n",
      "|         581|         BELIZE|       84|         BZ|        BLZ|  ISO 3166-2:BZ|             Belize|  Americas|Latin America and...|           19.0|              419.0|\n",
      "+------------+---------------+---------+-----------+-----------+---------------+-------------------+----------+--------------------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table\n",
    "country_codes_i94_df_spark_joined.createOrReplaceTempView(\"countries_table_DF\")\n",
    "countries_table = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT i94_cit          AS country_code,\n",
    "                        i94_country_name AS country_name,\n",
    "                        iso_country_code AS iso_ccode,\n",
    "                        alpha_2          AS iso_alpha_2,\n",
    "                        alpha_3          AS iso_alpha_3,\n",
    "                        iso_3166_2       AS iso_3166_2_code,\n",
    "                        name             AS iso_country_name,\n",
    "                        region           AS iso_region,\n",
    "                        sub_region       AS iso_sub_region,\n",
    "                        region_code      AS iso_region_code,\n",
    "                        sub_region_code  AS iso_sub_region_code\n",
    "        FROM countries_table_DF          AS countries\n",
    "        ORDER BY country_name\n",
    "    \n",
    "\"\"\")\n",
    "countries_table.printSchema()\n",
    "countries_table.show(20)\n",
    "countries_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- country_id: integer (nullable = true)\n",
      "\n",
      "+-----+----+-----+-------+----------+----------+----------+\n",
      "|cicid|year|month|i94port|state_code|airport_id|country_id|\n",
      "+-----+----+-----+-------+----------+----------+----------+\n",
      "|  7.0|2016|    4|    ATL|        AL|       ATL|       254|\n",
      "| 15.0|2016|    4|    WAS|        MI|       WAS|       101|\n",
      "| 16.0|2016|    4|    NYC|        MA|       NYC|       101|\n",
      "| 17.0|2016|    4|    NYC|        MA|       NYC|       101|\n",
      "| 18.0|2016|    4|    NYC|        MI|       NYC|       101|\n",
      "+-----+----+-----+-------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_I94_Data.printSchema()\n",
    "new_I94_Data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_code: string (nullable = false)\n",
      " |-- country_name: string (nullable = false)\n",
      " |-- iso_ccode: string (nullable = false)\n",
      " |-- iso_alpha_2: string (nullable = false)\n",
      " |-- iso_alpha_3: string (nullable = false)\n",
      " |-- iso_3166_2_code: string (nullable = false)\n",
      " |-- iso_country_name: string (nullable = false)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- iso_sub_region: string (nullable = true)\n",
      " |-- iso_region_code: string (nullable = true)\n",
      " |-- iso_sub_region_code: string (nullable = true)\n",
      "\n",
      "+------------+------------+---------+-----------+-----------+---------------+----------------+----------+------------------+---------------+-------------------+\n",
      "|country_code|country_name|iso_ccode|iso_alpha_2|iso_alpha_3|iso_3166_2_code|iso_country_name|iso_region|    iso_sub_region|iso_region_code|iso_sub_region_code|\n",
      "+------------+------------+---------+-----------+-----------+---------------+----------------+----------+------------------+---------------+-------------------+\n",
      "|         236| AFGHANISTAN|        4|         AF|        AFG|  ISO 3166-2:AF|     Afghanistan|      Asia|     Southern Asia|          142.0|               34.0|\n",
      "|         101|     ALBANIA|        8|         AL|        ALB|  ISO 3166-2:AL|         Albania|    Europe|   Southern Europe|          150.0|               39.0|\n",
      "|         316|     ALGERIA|       12|         DZ|        DZA|  ISO 3166-2:DZ|         Algeria|    Africa|   Northern Africa|            2.0|               15.0|\n",
      "|         102|     ANDORRA|       20|         AD|        AND|  ISO 3166-2:AD|         Andorra|    Europe|   Southern Europe|          150.0|               39.0|\n",
      "|         324|      ANGOLA|       24|         AO|        AGO|  ISO 3166-2:AO|          Angola|    Africa|Sub-Saharan Africa|            2.0|              202.0|\n",
      "+------------+------------+---------+-----------+-----------+---------------+----------------+----------+------------------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_table.printSchema()\n",
    "countries_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94_port: string (nullable = false)\n",
      " |-- i94_airport_name: string (nullable = false)\n",
      " |-- i94_airport_state: string (nullable = false)\n",
      "\n",
      "+--------+--------------------+-----------------+\n",
      "|i94_port|    i94_airport_name|i94_airport_state|\n",
      "+--------+--------------------+-----------------+\n",
      "|     ALC|               ALCAN|               AK|\n",
      "|     ANC|           ANCHORAGE|               AK|\n",
      "|     BAR|BAKER AAF - BAKER...|               AK|\n",
      "|     DAC|       DALTONS CACHE|               AK|\n",
      "|     PIZ|DEW STATION PT LA...|               AK|\n",
      "+--------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_airport_data.printSchema()\n",
    "new_airport_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- avg_temp_celcius: double (nullable = true)\n",
      " |-- avg_temp_fahrenheit: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- i94_cit: integer (nullable = true)\n",
      " |-- i94_country_name: string (nullable = false)\n",
      " |-- iso_country_code: string (nullable = false)\n",
      "\n",
      "+----+-----+----------------+-------------------+-------+-------+----------------+----------------+\n",
      "|year|month|avg_temp_celcius|avg_temp_fahrenheit|Country|i94_cit|i94_country_name|iso_country_code|\n",
      "+----+-----+----------------+-------------------+-------+-------+----------------+----------------+\n",
      "|2013|    7|            21.8|               71.2|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    9|            null|               null|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    8|            21.5|               70.8|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    2|             0.6|               33.1|ARMENIA|    151|         ARMENIA|              51|\n",
      "|2013|    4|            10.5|               50.9|ARMENIA|    151|         ARMENIA|              51|\n",
      "+----+-----+----------------+-------------------+-------+-------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_Temperatures_joined.printSchema()\n",
    "new_Temperatures_joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Joint_imm_tables = new_I94_Data.join(new_Temperatures_joined,\\\n",
    "                           (new_I94_Data.country_id == new_Temperatures_joined.i94_cit) & \\\n",
    "                            (new_I94_Data.month == new_Temperatures_joined.month))\\\n",
    "                        .join(countries_table,\\\n",
    "                             (new_I94_Data.country_id == countries_table.country_code))\\\n",
    "                         .join(new_airport_data,\\\n",
    "                              (new_I94_Data.airport_id == new_airport_data.i94_port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allow unlimited time for SQL joins and parquet writes.\n",
    "sqlContext.setConf(\"spark.sql.autoBroadcastJoinThreshold\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- avg_temp_fahrenheit: double (nullable = true)\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- country_id: integer (nullable = true)\n",
      " |-- country_name: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Joint_imm_tables.createOrReplaceTempView(\"immigrations_table\")\n",
    "immigrations_table = spark.sql(\"\"\"\n",
    "           SELECT    cicid,\n",
    "                     avg_temp_fahrenheit,\n",
    "                     airport_id,\n",
    "                     country_id,\n",
    "                     country_name\n",
    "                     \n",
    "                     \n",
    "           FROM immigrations_table\n",
    "\"\"\")\n",
    "immigrations_table.printSchema()\n",
    "# immigrations_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigrations_table.toDF('cicid', 'avg_temp_fahrenheit', 'airport_id', 'country_id', \\\n",
    "          'country_name').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write fact table to parquet\n",
    "immigrations_table.write.parquet(\"immigrations_table\")\n",
    "print(\"Done with writing fact table to parquet file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------+--------------------+--------------------+----------------------+\n",
      "|(cicid IS NULL)|(avg_temp_fahrenheit IS NULL)|(airport_id IS NULL)|(country_id IS NULL)|(country_name IS NULL)|\n",
      "+---------------+-----------------------------+--------------------+--------------------+----------------------+\n",
      "|          false|                        false|               false|               false|                 false|\n",
      "+---------------+-----------------------------+--------------------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for NULL values in 'cicid', 'avg_temp_fahrenheit', 'airport_id', 'country_id', 'country_name'\n",
    "# If false is retured for all columns selected then the data is fine across the dataset\n",
    "immigrations_table.select(isnull('cicid'),\\\n",
    "                             isnull('avg_temp_fahrenheit'),\\\n",
    "                             isnull('airport_id'),\\\n",
    "                             isnull('country_id'),\\\n",
    "                             isnull('country_name')).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2396484|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(*) FROM immigrations_table').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. \n",
    "- See the DataDictionary.md file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. I used Python, Pandas and Apache Spark to read, transform, and create data outputs for further analysis for this project, because of the he small amount of data and the speed of Spark.\n",
    "\n",
    "### 2. ETL script should be run quarterly basis (assuming that new I94 data is available once per quarterly). This gives the most up-to-date data for government and organizations.\n",
    "\n",
    "### 3. Under the following scenarios, I would approach the problem differently:\n",
    "    \n",
    "    - If the data was increased by 100x, Input data should be stoted in cloud storage e.g. AWS S3,    \n",
    "    Clustered Spark will be used to enable parallel processing the data for faster processing.\n",
    "    Clustered Cloud DB e.g. AWS Redshift should be used to store the data during the processing (staging and final tables).\n",
    "    Output data (parquet files) should be stored to Cloud storage e.g. AWS S3 for easy access or to a Cloud DB for further analysis. \n",
    "\n",
    "    \n",
    "    - To update on a daily basis I would use Apache Airflow to create a schedule to run a distributed update on all tables with data streamed from the source.\n",
    "    \n",
    "    - If the data needs to be accessed by 100+ people, I would use a web app running on Amazon AWS for increased capacity.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
